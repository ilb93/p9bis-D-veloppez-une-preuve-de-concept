import streamlit as st
import pandas as pd
import numpy as np
import joblib
from pathlib import Path
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter

# ======================================================
# CONFIG STREAMLIT
# ======================================================
st.set_page_config(
    page_title="Proof of Concept ‚Äì Scoring de risque de cr√©dit",
    layout="wide"
)

st.title("üìä Proof of Concept ‚Äì Scoring de risque de d√©faut de remboursement")

st.markdown(
    """
Cette application pr√©sente une **preuve de concept** de scoring de risque bas√©e sur un mod√®le **LightGBM**.

- Les **graphiques** affichent des **valeurs m√©tier lisibles (ann√©es / euros)**  
- La **pr√©diction** utilise **les features exactement attendues par le mod√®le**  
"""
)

# ======================================================
# CHARGEMENT MOD√àLE
# ======================================================
@st.cache_resource
def load_model():
    return joblib.load(Path("artifacts") / "lgbm.joblib")

model = load_model()

def get_expected_features(m):
    """
    R√©cup√®re la liste exacte des features attendues par LightGBM.
    """
    if hasattr(m, "booster_") and m.booster_ is not None:
        return list(m.booster_.feature_name())
    if hasattr(m, "feature_name_"):
        return list(m.feature_name_)
    # Fallback (rare)
    raise RuntimeError("Impossible de r√©cup√©rer la liste des features attendues par le mod√®le.")

EXPECTED_FEATURES = get_expected_features(model)

# ======================================================
# UPLOAD CSV UNIQUE (comme √† l'origine)
# ======================================================
st.subheader("üìÇ Import du fichier CSV (unique)")

uploaded_file = st.file_uploader(
    "Importer le fichier CSV unifi√© (ex: sample_unified.csv)",
    type=["csv"]
)

if uploaded_file is None:
    st.info("Veuillez importer un fichier CSV pour continuer.")
    st.stop()

df = pd.read_csv(uploaded_file)
df.columns = [c.strip() for c in df.columns]

st.success("Fichier charg√© avec succ√®s")
st.write(f"Lignes : {df.shape[0]} | Colonnes : {df.shape[1]}")
st.dataframe(df.head())

# ======================================================
# OUTILS : gestion colonnes dupliqu√©es et s√©lection "humaine"
# ======================================================
def euro_fmt(x, pos=None):
    try:
        return f"{x:,.0f} ‚Ç¨".replace(",", " ")
    except Exception:
        return str(x)

def pick_best_human_column(base_name: str, data: pd.DataFrame, money=False):
    """
    Ton CSV peut contenir AMT_CREDIT + AMT_CREDIT.1 etc.
    On choisit la version 'humaine' (= valeurs en euros, pas z-score).
    Heuristique :
      - on prend la colonne dont la m√©diane absolue est la plus grande
      - (les z-scores ont une m√©diane proche de 0)
    """
    candidates = [c for c in data.columns if c == base_name or c.startswith(base_name + ".")]
    if not candidates:
        return None

    best_col = None
    best_score = -np.inf

    for c in candidates:
        s = pd.to_numeric(data[c], errors="coerce")
        med = np.nanmedian(np.abs(s.values))
        # petit bonus si money attendu (souvent > 1000)
        score = med + (1000 if (money and med > 1000) else 0)
        if score > best_score:
            best_score = score
            best_col = c

    return best_col

def clean_employment_years(series: pd.Series) -> pd.Series:
    """
    Corrige le probl√®me logique qui te d√©truit l'histogramme :
    - 1000 = valeur sentinelle (emploi inconnu / anomalie) -> NaN
    - valeurs n√©gatives -> 0 (pas d'anciennet√©)
    """
    s = pd.to_numeric(series, errors="coerce")
    s = s.where(~(s >= 900), np.nan)     # 1000 -> NaN
    s = s.where(s >= 0, 0)              # n√©gatif -> 0
    return s

def clean_age_years(series: pd.Series) -> pd.Series:
    s = pd.to_numeric(series, errors="coerce")
    # on prot√®ge des √¢ges absurdes
    s = s.where((s >= 0) & (s <= 120), np.nan)
    return s

def clean_money(series: pd.Series) -> pd.Series:
    s = pd.to_numeric(series, errors="coerce")
    s = s.where(s >= 0, np.nan)  # montants n√©gatifs => NaN
    return s

# ======================================================
# CONSTRUCTION DES 5 VARIABLES "HUMAINES"
# ======================================================
st.subheader("üîç Variables m√©tiers (lisibles)")

age_col = "age_years" if "age_years" in df.columns else None
emp_col = "employment_years" if "employment_years" in df.columns else None

credit_col = pick_best_human_column("AMT_CREDIT", df, money=True)
goods_col  = pick_best_human_column("AMT_GOODS_PRICE", df, money=True)
annuity_col = pick_best_human_column("AMT_ANNUITY", df, money=True)

missing_human = []
if age_col is None: missing_human.append("age_years")
if emp_col is None: missing_human.append("employment_years")
if credit_col is None: missing_human.append("AMT_CREDIT")
if goods_col is None: missing_human.append("AMT_GOODS_PRICE")
if annuity_col is None: missing_human.append("AMT_ANNUITY")

if missing_human:
    st.warning(
        "Certaines colonnes 'humaines' ne sont pas trouv√©es : "
        + ", ".join(missing_human)
        + "\n\n‚û°Ô∏è Les graphes ne pourront pas √™tre complets tant que le CSV ne contient pas ces colonnes."
    )

# dataframe humain pour graphes
human_df = pd.DataFrame()

if age_col:
    human_df["√Çge (ann√©es)"] = clean_age_years(df[age_col])
if emp_col:
    human_df["Anciennet√© emploi (ann√©es)"] = clean_employment_years(df[emp_col])
if credit_col:
    human_df["Montant du cr√©dit (‚Ç¨)"] = clean_money(df[credit_col])
if goods_col:
    human_df["Prix du bien (‚Ç¨)"] = clean_money(df[goods_col])
if annuity_col:
    human_df["Annuit√© du cr√©dit (‚Ç¨)"] = clean_money(df[annuity_col])

st.write("Colonnes utilis√©es pour les graphes :")
st.code(
    "\n".join([
        f"age_years -> {age_col}",
        f"employment_years -> {emp_col}",
        f"AMT_CREDIT -> {credit_col}",
        f"AMT_GOODS_PRICE -> {goods_col}",
        f"AMT_ANNUITY -> {annuity_col}",
    ])
)

# ======================================================
# ANALYSE EXPLORATOIRE (GRAPHIQUES HUMAINS)
# ======================================================
st.subheader("üìä Analyse exploratoire ‚Äì donn√©es population (valeurs humaines)")

if human_df.shape[1] == 0:
    st.stop()

var_label = st.selectbox("Choisir une variable", list(human_df.columns))
series = human_df[var_label].dropna()

col_plot, col_info = st.columns([2, 1])

with col_plot:
    fig, ax = plt.subplots(figsize=(9, 4))

    # Histogramme
    ax.hist(series.values, bins=30, edgecolor="black")
    ax.set_title(f"Distribution ‚Äî {var_label}")
    ax.set_xlabel(var_label)
    ax.set_ylabel("Nombre d'individus")

    # Format ‚Ç¨ si besoin
    if "‚Ç¨" in var_label:
        ax.xaxis.set_major_formatter(FuncFormatter(euro_fmt))

    st.pyplot(fig)

with col_info:
    st.markdown("### üìå Rep√®res")
    if len(series) == 0:
        st.write("Aucune donn√©e exploitable.")
    else:
        vmin = float(np.nanmin(series.values))
        vmed = float(np.nanmedian(series.values))
        vmax = float(np.nanmax(series.values))

        if "‚Ç¨" in var_label:
            st.metric("Min", euro_fmt(vmin))
            st.metric("M√©diane", euro_fmt(vmed))
            st.metric("Max", euro_fmt(vmax))
        else:
            st.metric("Min", f"{vmin:.1f}")
            st.metric("M√©diane", f"{vmed:.1f}")
            st.metric("Max", f"{vmax:.1f}")

    st.markdown("### ‚ÑπÔ∏è Interpr√©tation")
    if "√Çge" in var_label:
        st.info("√Çge r√©el (ann√©es). Valeur directement interpr√©table.")
    elif "Anciennet√©" in var_label:
        st.info(
            "Anciennet√© r√©elle (ann√©es). "
            "‚ö†Ô∏è La valeur 1000 a √©t√© trait√©e comme 'inconnue' (sinon elle casse l'histogramme)."
        )
    else:
        st.info("Variable mon√©taire r√©elle (euros). Distribution souvent asym√©trique (queue √† droite).")

# ======================================================
# PR√âDICTION (features mod√®le)
# ======================================================
st.subheader("üéØ S√©lection d‚Äôun individu + pr√©diction")

row_id = st.slider(
    "Choisir un individu",
    min_value=0,
    max_value=len(df) - 1,
    value=0
)

# On construit X_row STRICTEMENT selon les features attendues par le mod√®le
def build_model_row(data: pd.DataFrame, idx: int, expected_features: list[str]) -> pd.DataFrame:
    """
    Reconstruit une ligne mod√®le avec EXACTEMENT :
    - les colonnes attendues
    - le bon ordre
    - les bons dtypes num√©riques
    - en g√©rant les colonnes suffix√©es (.1, .2...) si ton CSV a eu des collisions
    """
    # mapping feature -> colonne r√©elle existante
    colset = set(data.columns)

    mapping = {}
    for f in expected_features:
        if f in colset:
            mapping[f] = f
        else:
            # tente f.1 / f.2 / f.3 ...
            found = None
            for k in range(1, 6):
                cand = f"{f}.{k}"
                if cand in colset:
                    found = cand
                    break
            mapping[f] = found  # peut √™tre None

    # Construire la ligne
    row_dict = {}
    for f in expected_features:
        src = mapping[f]
        if src is None:
            row_dict[f] = 0.0
        else:
            val = pd.to_numeric(data.loc[idx, src], errors="coerce")
            row_dict[f] = 0.0 if pd.isna(val) else float(val)

    X_row = pd.DataFrame([row_dict], columns=expected_features)
    return X_row

X_row = build_model_row(df, row_id, EXPECTED_FEATURES)

# affichage "humain" de l'individu (les 5 variables)
st.markdown("### üë§ Profil (variables humaines)")
human_profile = {}
for k in human_df.columns:
    v = human_df.loc[row_id, k] if row_id in human_df.index else np.nan
    human_profile[k] = v
human_profile_df = pd.DataFrame([human_profile])

# format plus lisible dans le tableau
for c in human_profile_df.columns:
    if "‚Ç¨" in c:
        human_profile_df[c] = human_profile_df[c].apply(lambda x: euro_fmt(x) if pd.notna(x) else "")
    else:
        human_profile_df[c] = human_profile_df[c].apply(lambda x: f"{x:.1f}" if pd.notna(x) else "")

st.dataframe(human_profile_df, use_container_width=True)

# pr√©diction
st.subheader("üìà R√©sultat de la pr√©diction")

try:
    proba = float(model.predict_proba(X_row)[0][1])
    prediction = int(proba >= 0.5)

    c1, c2 = st.columns(2)
    with c1:
        st.metric("Classe pr√©dite", "Risque de d√©faut" if prediction == 1 else "Pas de d√©faut")
    with c2:
        st.metric("Probabilit√© de d√©faut", f"{proba:.2%}")

except Exception as e:
    st.error(
        "Erreur lors de la pr√©diction. "
        "Cause la plus fr√©quente : colonnes/features non align√©es avec le mod√®le.\n\n"
        f"D√©tail : {type(e).__name__} ‚Äî {e}"
    )
    st.stop()

# ======================================================
# CONCLUSION
# ======================================================
st.subheader("‚úÖ Conclusion")

st.markdown(
    """
- ‚úÖ Graphiques bas√©s sur des **valeurs m√©tiers lisibles**
- ‚úÖ Pr√©diction bas√©e sur les **features attendues par LightGBM**
- ‚úÖ 1 seul fichier CSV (comme au d√©part)

Si tu as encore une erreur LightGBM, c‚Äôest **forc√©ment** un probl√®me d‚Äôalignement des features :  
ce code reconstruit justement **les colonnes attendues** (m√™me si le CSV a `AMT_CREDIT.1`, etc.).
"""
)
