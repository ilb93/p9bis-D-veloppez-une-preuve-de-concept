import streamlit as st
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import pickle
from pathlib import Path
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
import time
time.sleep(15)

# ======================================================
# CONFIG STREAMLIT
# ======================================================
st.set_page_config(
    page_title="Proof of Concept â€“ Scoring de risque de crÃ©dit",
    layout="wide"
)

st.title("ðŸ“Š Proof of Concept â€“ Scoring de risque de dÃ©faut de remboursement")

st.markdown(
    """
Cette application prÃ©sente une **preuve de concept** de scoring de risque basÃ©e sur un modÃ¨le **SAINT Transformer**.

- Les **graphiques** affichent des **valeurs mÃ©tier lisibles (annÃ©es / euros)**  
- La **prÃ©diction** utilise **exactement les variables attendues par le modÃ¨le**
- La **dÃ©cision est fondÃ©e sur un modÃ¨le Deep Learning tabulaire**
"""
)

# ======================================================
# CHARGEMENT MODÃˆLE SAINT
# ======================================================

@st.cache_resource
def load_saint():

    with open("models/saint_config.pkl", "rb") as f:
        config = pickle.load(f)

    with open("models/saint_metadata.pkl", "rb") as f:
        metadata = pickle.load(f)

    with open("models/saint_threshold.pkl", "rb") as f:
        threshold = pickle.load(f)

    input_dim = len(metadata["numerical_columns"])

    class SimpleSaint(nn.Module):
        def __init__(self, input_dim, dim, depth):
            super().__init__()
            layers = []
            current_dim = input_dim
            for _ in range(depth):
                layers.append(nn.Linear(current_dim, dim))
                layers.append(nn.ReLU())
                current_dim = dim
            layers.append(nn.Linear(dim, 1))
            self.network = nn.Sequential(*layers)

        def forward(self, x):
            return self.network(x)

    model = SimpleSaint(
        input_dim=input_dim,
        dim=config["dim"],
        depth=config["depth"]
    )

    state_dict = torch.load("models/saint_weights.pth", map_location="cpu")
    model.load_state_dict(state_dict, strict=False)
    model.eval()

    return model, metadata, threshold


model, metadata, THRESHOLD = load_saint()
EXPECTED_FEATURES = metadata["numerical_columns"]

# ======================================================
# UPLOAD CSV
# ======================================================
st.subheader("ðŸ“‚ Import du fichier CSV")

uploaded_file = st.file_uploader(
    "Importer le fichier CSV unifiÃ© (ex : sample_unified.csv)",
    type=["csv"]
)

if uploaded_file is None:
    st.stop()

df = pd.read_csv(uploaded_file)
df.columns = [c.strip() for c in df.columns]

st.success("Fichier chargÃ© avec succÃ¨s")
st.write(f"Lignes : {df.shape[0]} | Colonnes : {df.shape[1]}")

st.markdown("### ðŸ“ˆ Statistiques descriptives")
st.dataframe(df.describe().T, use_container_width=True)

# ======================================================
# FORMATAGE
# ======================================================

def euro_fmt(x, pos=None):
    try:
        return f"{x:,.0f} â‚¬".replace(",", " ")
    except Exception:
        return ""

# ======================================================
# VARIABLES MÃ‰TIER
# ======================================================

human_df = df.copy()

# ======================================================
# ANALYSE EXPLORATOIRE
# ======================================================

st.subheader("ðŸ“Š Analyse exploratoire â€“ population")

var_label = st.selectbox("Choisir une variable", EXPECTED_FEATURES)
series = pd.to_numeric(df[var_label], errors="coerce").dropna()

col_plot, col_info = st.columns([2, 1])

with col_plot:
    fig, ax = plt.subplots(figsize=(9, 4))
    ax.hist(series, bins=30, edgecolor="black")
    ax.set_title(f"Distribution â€” {var_label}")
    ax.set_xlabel(var_label)
    ax.set_ylabel("Nombre d'individus")
    st.pyplot(fig)

with col_info:
    st.metric("Min", f"{series.min():.2f}")
    st.metric("MÃ©diane", f"{series.median():.2f}")
    st.metric("Max", f"{series.max():.2f}")

# ======================================================
# SÃ‰LECTION INDIVIDU
# ======================================================

st.subheader("ðŸŽ¯ SÃ©lection dâ€™un individu")

row_id = st.slider("Choisir un individu", 0, len(df) - 1, 0)

# ======================================================
# PRÃ‰PARATION DONNÃ‰ES MODÃˆLE
# ======================================================

def build_model_row(data, idx, expected):
    row = []
    for f in expected:
        v = pd.to_numeric(data.loc[idx, f], errors="coerce")
        row.append(0.0 if pd.isna(v) else float(v))
    return torch.tensor([row], dtype=torch.float32)

X_row = build_model_row(df, row_id, EXPECTED_FEATURES)

# ======================================================
# PRÃ‰DICTION SAINT
# ======================================================

with torch.no_grad():
    output = model(X_row)
    proba = torch.sigmoid(output).item()

if proba < THRESHOLD:
    verdict = "Faible risque de crÃ©dit"
elif proba < THRESHOLD + 0.2:
    verdict = "Risque de crÃ©dit modÃ©rÃ©"
else:
    verdict = "Risque de crÃ©dit Ã©levÃ©"

st.subheader("ðŸ“ˆ RÃ©sultat de la prÃ©diction")

c1, c2 = st.columns(2)
c1.metric("Ã‰valuation du profil", verdict)
c2.metric("ProbabilitÃ© de dÃ©faut", f"{proba:.2%}")

# ======================================================
# CONCLUSION
# ======================================================

st.subheader("âœ… Conclusion")

st.markdown(
    """
Cette preuve de concept dÃ©montre une **approche Deep Learning du scoring de crÃ©dit**, 
tout en conservant une interface mÃ©tier claire et exploitable.
"""
)
